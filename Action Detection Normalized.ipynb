{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'grad (Python 3.8.20)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==2.7.1 tensorflow-gpu==2.7.1 opencv-python==4.5.5.64 mediapipe==0.10.5 scikit-learn==0.24.2 numpy==1.19.5 matplotlib       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 16:19:30.498021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744982371.616934    2272 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744982371.918367    2272 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744982374.490536    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744982374.490564    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744982374.490566    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744982374.490567    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=1, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=1, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=1, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=1, circle_radius=1)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1) ### chage to your device usually 0\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # print(frame.shape)\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(results)\n",
    "        \n",
    "        # # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path(r'MP_Data') ### change your path\n",
    "\n",
    "# Actions that we try to detect\n",
    "# actions = np.array(['null','besm allah' , 'alsalam alekom' , 'alekom salam' , 'aslan w shlan' , 'me',\n",
    "#                     'age','alhamdulilah' , 'bad' , 'how are you' , 'friend' ,\n",
    "#                     'good' , 'happy' , 'you' , 'my name is' , 'no' , \n",
    "#                     'or' , 'taaban' , 'what' , 'where' , 'yes' ,\n",
    "#                     'look' , 'said' , 'walking' , 'did not hear' , 'remind me',\n",
    "#                     'eat' , 'bayt' , 'hospital' , 'run' , 'sleep',\n",
    "#                     'think' , 'tomorrow' , 'yesterday' , 'today' , 'when',\n",
    "#                     'dhuhr' , 'sabah' , 'university' , 'kuliyah' ,'night',\n",
    "#                     'a3ooth bellah' , 'danger' , 'enough' , 'hot' , 'mosque' , 'surprise' , 'tard' , \n",
    "#                     'big' , 'clean' , 'dirty' , 'fire' , 'give me' , 'sho dakhalak' , 'small' , \n",
    "#                     'help' , 'same' , 'hour' , 'important' , 'ok' , 'please' , 'want' ,\n",
    "#                     'riyadah' , 'sallah' , 'telephone' , 'hamam' , 'water' , 'eid'\n",
    "#                    ])\n",
    "\n",
    "actions = np.array(['null','besm allah' , 'alsalam alekom' , 'alekom salam' , 'ahlan w shlan' , 'me',\n",
    "                    'age','alhamdulilah' , 'bad' , 'how are you' , 'friend' ,\n",
    "                    'good' , 'happy' , 'you' , 'my name is' , 'no' , \n",
    "                    'or' , 'taaban' , 'what' , 'where' , 'yes' ])\n",
    "\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 60\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 45\n",
    "\n",
    "# Folder start\n",
    "start_folder = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    \n",
    "    for sequence in range(start_folder,no_sequences+1):\n",
    "        try: \n",
    "            os.makedirs(Path.joinpath(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736345292.780685   65453 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3\n",
      "W0000 00:00:1736345292.828325  189157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.842201  189156 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.843696  189155 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.843698  189160 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.843727  189159 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.846394  189159 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.850580  189158 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736345292.852455  189157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) ### chage to your device usually 0\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder, start_folder+no_sequences + 1):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(3000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = Path.joinpath(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        cv2.waitKey(3000)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'null': 0,\n",
       " 'big': 1,\n",
       " 'clean': 2,\n",
       " 'dirty': 3,\n",
       " 'fire': 4,\n",
       " 'give me': 5,\n",
       " 'sho dakhalak': 6,\n",
       " 'small': 7}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks, epsilon=1e-6):\n",
    "    num_points = len(landmarks) // 3\n",
    "    landmarks = landmarks.reshape(num_points, 3)\n",
    "\n",
    "    # Use wrist as anchor for hands and midpoint of hips for pose\n",
    "    if num_points == 33:  # Pose\n",
    "        anchor = landmarks[23]  # Left hip\n",
    "        reference_dist = np.linalg.norm(landmarks[11] - landmarks[12])  # Shoulder width\n",
    "    elif num_points == 21:  # Hands\n",
    "        anchor = landmarks[0]  # Wrist\n",
    "        reference_dist = np.linalg.norm(landmarks[5] - landmarks[17])  # Palm width\n",
    "    else:\n",
    "        return landmarks.flatten()  # Return unchanged for unexpected data\n",
    "\n",
    "    # Handle potential zero or small reference distance\n",
    "    if reference_dist < epsilon:\n",
    "        return np.zeros_like(landmarks.flatten())  # Return zeroed-out array for invalid frames\n",
    "\n",
    "    # Translate (center around anchor)\n",
    "    landmarks -= anchor\n",
    "\n",
    "    # Scale (normalize distances)\n",
    "    landmarks /= reference_dist\n",
    "\n",
    "    return landmarks.flatten()\n",
    "\n",
    "\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))):\n",
    "        try:\n",
    "            sequence = sequence.astype(int)\n",
    "            window = []\n",
    "            for frame_num in range(sequence_length):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "                normalized_res = np.concatenate([\n",
    "                    normalize_landmarks(res[:33*4]),  # Pose landmarks\n",
    "                    normalize_landmarks(res[33*4:33*4 + 21*3]),  # Left hand landmarks\n",
    "                    normalize_landmarks(res[33*4 + 21*3:])  # Right hand landmarks\n",
    "                ])\n",
    "                window.append(normalized_res)\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in sequence {sequence}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488, 45, 258)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488, 45, 258)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(45,258)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(256, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(128, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "13/13 [==============================] - 5s 90ms/step - loss: 1.0284 - categorical_accuracy: 0.6691\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.3791 - categorical_accuracy: 0.9010\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.1754 - categorical_accuracy: 0.9710\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.2204 - categorical_accuracy: 0.9493\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.1490 - categorical_accuracy: 0.9662\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.2027 - categorical_accuracy: 0.9541\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.1260 - categorical_accuracy: 0.9783\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1790 - categorical_accuracy: 0.9589\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.1067 - categorical_accuracy: 0.9807\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.0859 - categorical_accuracy: 0.9855\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1209 - categorical_accuracy: 0.9734\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0445 - categorical_accuracy: 0.9928\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0368 - categorical_accuracy: 0.9976\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0285 - categorical_accuracy: 0.9976\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0262 - categorical_accuracy: 0.9976\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.0260 - categorical_accuracy: 0.9976\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0241 - categorical_accuracy: 0.9976\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0241 - categorical_accuracy: 0.9976\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0225 - categorical_accuracy: 0.9976\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0218 - categorical_accuracy: 0.9976\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.0206 - categorical_accuracy: 0.9976\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0204 - categorical_accuracy: 0.9976\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0198 - categorical_accuracy: 0.9976\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0184 - categorical_accuracy: 0.9976\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0144 - categorical_accuracy: 0.9976\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0212 - categorical_accuracy: 0.9976\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1617 - categorical_accuracy: 0.9734\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.2350 - categorical_accuracy: 0.9541\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.1833 - categorical_accuracy: 0.9541\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.1703 - categorical_accuracy: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f21b29f490>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 45, 64)            82688     \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 45, 128)           98816     \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 45, 256)           394240    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,976\n",
      "Trainable params: 799,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[62,  0],\n",
       "        [ 0, 12]],\n",
       "\n",
       "       [[61,  2],\n",
       "        [ 0, 11]],\n",
       "\n",
       "       [[65,  0],\n",
       "        [ 0,  9]],\n",
       "\n",
       "       [[67,  0],\n",
       "        [ 0,  7]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 5,  9]],\n",
       "\n",
       "       [[70,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[65,  0],\n",
       "        [ 0,  9]],\n",
       "\n",
       "       [[63,  3],\n",
       "        [ 0,  8]]], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9324324324324325"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(16,137,205), (16,17,245), (122,147,45), (156,117,240)]*15\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = [(245,117,16), (117,245,16), (16,117,245),(16,137,205), (16,17,245), (122,147,45), (156,117,240)]*15\n",
    "\n",
    "# def prob_viz(res, actions, input_frame, colors):\n",
    "#     output_frame = input_frame.copy()\n",
    "#     column_width = 200  # Adjust the width between columns\n",
    "#     for num, prob in enumerate(res):\n",
    "#         # Determine the column and row for the current word\n",
    "#         column = num // 20  # Divide into columns (0 for first, 1 for second)\n",
    "#         row = num % 20  # Index within the column\n",
    "        \n",
    "#         # Calculate x and y positions for rectangles and text\n",
    "#         x_start = column * column_width\n",
    "#         y_start = 60 + row * 40\n",
    "#         x_end = x_start + int(prob * 100)\n",
    "#         y_end = y_start + 30\n",
    "        \n",
    "#         # Draw the probability bar\n",
    "#         cv2.rectangle(output_frame, (x_start, y_start), (x_end, y_end), colors[num], -1)\n",
    "        \n",
    "#         # Draw the action label\n",
    "#         cv2.putText(output_frame, actions[num], (x_start, y_end - 5), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "#     return output_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcap\u001b[49m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        # Determine the column (0 or 1) and row\n",
    "        col = num % 2\n",
    "        row = num // 2\n",
    "\n",
    "        # Adjust x-offset and y-offset for two columns\n",
    "        x_offset = col * 320  # Column spacing (width of each column)\n",
    "        y_offset = 60 + row * 40  # Row spacing\n",
    "\n",
    "        # Draw the rectangles and text\n",
    "        cv2.rectangle(output_frame, \n",
    "                      (x_offset, y_offset), \n",
    "                      (x_offset + int(prob * 300), y_offset + 30), \n",
    "                      colors[num], -1)\n",
    "        cv2.putText(output_frame, \n",
    "                    actions[num], \n",
    "                    (x_offset + 10, y_offset + 25),  # Adjusted to align with the rectangles\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return output_frame\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "me\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "null\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "alhamdulilah\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "me\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n",
      "alhamdulilah\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "def normalize_landmarks(landmarks, epsilon=1e-6):\n",
    "    num_points = len(landmarks) // 3\n",
    "    landmarks = landmarks.reshape(num_points, 3)\n",
    "\n",
    "    if num_points == 33:  # Pose\n",
    "        anchor = landmarks[23]  # Left hip\n",
    "        reference_dist = np.linalg.norm(landmarks[11] - landmarks[12])  # Shoulder width\n",
    "    elif num_points == 21:  # Hands\n",
    "        anchor = landmarks[0]  # Wrist\n",
    "        reference_dist = np.linalg.norm(landmarks[5] - landmarks[17])  # Palm width\n",
    "    else:\n",
    "        return landmarks.flatten()  # Return unchanged for unexpected data\n",
    "\n",
    "    # Handle potential zero or small reference distance\n",
    "    if reference_dist < epsilon:\n",
    "        return np.zeros_like(landmarks.flatten())\n",
    "\n",
    "    landmarks -= anchor\n",
    "    landmarks /= reference_dist\n",
    "\n",
    "    return landmarks.flatten()\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # Normalize keypoints\n",
    "    normalized_pose = normalize_landmarks(pose[:33*4])\n",
    "    normalized_lh = normalize_landmarks(lh)\n",
    "    normalized_rh = normalize_landmarks(rh)\n",
    "    \n",
    "    return np.concatenate([normalized_pose, normalized_lh, normalized_rh])\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "# set to 30 fps\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (640*2, 480*2))\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(results)\n",
    "\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-45:]\n",
    "\n",
    "        if len(sequence) == 45:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            sentence = []\n",
    "\n",
    "        cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting for hello, video 1\n",
      "\n",
      "Collecting for hello, video 2\n",
      "\n",
      "Collecting for hello, video 3\n",
      "\n",
      "Collecting for hello, video 4\n",
      "\n",
      "Collecting for hello, video 5\n",
      "\n",
      "Collecting for thanks, video 1\n",
      "\n",
      "Collecting for thanks, video 2\n",
      "\n",
      "Collecting for thanks, video 3\n",
      "\n",
      "Collecting for thanks, video 4\n",
      "\n",
      "Collecting for thanks, video 5\n",
      "\n",
      "Collecting for iloveyou, video 1\n",
      "\n",
      "Collecting for iloveyou, video 2\n",
      "\n",
      "Collecting for iloveyou, video 3\n",
      "\n",
      "Collecting for iloveyou, video 4\n",
      "\n",
      "Collecting for iloveyou, video 5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
